# Design

[TOC]

> Figure 1

![system_architecture](./system_architecture.png)

## 1、Hive Architecture

*Figure 1 shows the major components of Hive and its interactions with Hadoop. As shown in that figure, the main components of Hive are:*

Figure 1 展示了 Hive 的主要组件以及和 Hadoop 的交互。主要组件有:

- **UI：向系统提交查询和其他操作的用户界面**。在2011年，系统有一个命令行界面和一个基于 GUI 的 web 正在开发中。

*UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed.*

- **Driver：接受查询的组件**。该组件实现了会话句柄的概念，并提供了基于 JDBC/ODBC 接口的 api 执行和获取。

*Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces.*

- **Compiler：解析查询的组件**，对不同的查询块和查询表达式进行语义分析，最终利用从元数据存储中查找的表和分区元数据生成执行计划。

*Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore.*

- **Metastore：存储数仓中的表和分区的结构信息的组件**，包括列和列类型信息、读取和写入数据的序列化器和反序列化器，和对应的存储数据的 HDFS 文件。

*Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored.*

- **Execution Engine：执行由 Compiler 创建的执行计划**。该执行计划是一个 stages 的 DAG 图。执行引擎管理着执行计划的不同 stage 间的依赖关系，并在合适的系统组件上执行这些 stage。

*Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.*

*Figure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table's location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9).*

Figure 1 也展示了一个典型的查询流。 

**step 1**：UI 调用执行接口给 Driver 。

**step 2**：Driver为查询创建一个会话句柄，并发送查询到 Compiler ，以生成执行计划。

**step 3, 4**：Compiler 从元数据存储库里获取必要的元数据。该元数据对查询树中的表达式进行类型检查，以及基于查询谓词对分区进行修剪。

**step 5**：Compiler 生成的执行计划是一个 stages 的 DAG 图。每个 stage 都是一个 map/reduce job、元数据操作或 HDFS 上的操作。

对于 map/reduce stages，执行计划包含多个 map operator trees (在mappers上执行的操作树)和一个 reduce operator tree (需要reducers的操作)

**step 6**：Execution Engine 提交这些 stages 到合适的组件。(6.1, 6.2 and 6.3)

**step 7, 8, 9**：在每个任务(mapper/reducer)中，和表或中间输出结果相关的反序列化器被用来从 HDFS 文件中读取行数据，这些行数据再被传递到相关的操作树。

一旦生成了输出，就通过序列化器写入到临时 HDFS 文件中(这发生在mapper中，以防操作不需要reduce)。这个临时文件向执行计划的下一 map/reduce stages 提供数据。

对于 DML 操作，最终的临时文件被移动到表的对应路径下。这个 schema 用来确保脏数据不被读取。(在 HDFS 中，文件重命名是一个原子操作。)(DML，Data Manipulation Language 数据操纵语言)

对于查询，临时文件的内容由执行引擎直接从 HDFS 中读取，作为从 Driver 获取调用的一部分。

## 2、Hive Data Model

*Data in Hive is organized into:*

Hive 中的数据被组织成如下形式：

*Tables – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases.*

- **表**：类似于关系数据库中的表。可以 filter 、project 、 join 和 union 表。

	表中是所有数据是存储在 HDFS 的目录下的。

	Hive 支持外部表的概念，即通过为创建表语句 DDL 提供合适的位置，可以在 HDFS 中已存在的文件或目录下创建表。(DDL,数据库模式定义语言Data Definition Language)

	表中的行被组织成有类型的列，类似于关系数据库。

*Partitions – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the <table location>/ds=<date> directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = '2008-09-01' would only have to look at files in <table location>/ds=2008-09-01/ directory in HDFS.*

- **分区**：每个表有一个或多个分区 key ，这些 key 决定了数据如何被存储。例如，分区列是 ds 的表 T 具有包含特定日期数据的文件，这些文件存储在 HDFS 中的表路径 `ds=date` 的目录中。

	分区允许系统基于查询谓词对要检查的数据进行修剪，例如，如果查询表 T 满足 `T.ds = '2008-09-01'` 的行，则只需查看 HDFS 中 `/ds=2008-09-01/` 目录中的文件。

*Buckets – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table).*

- **分桶**：通过计算表的某一列的 hash 值，分区中的数据可以再被划分到桶中。每个桶在分区目录下以文件形式存储。分桶允许系统有效地评估依赖于样本数据的查询(这些查询使用表上的 sample 子句)。

*Apart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe (Serailization/Deserialization) and object inspector interfaces. User can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files). These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Builtin object inspectors like ListObjectInspector, StructObjectInspector and MapObjectInspector provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful builtin functions like size and index operators are provided. The dotted notation is used to navigate nested types, for example a.b.c = 1 looks at field c of field b of type a and compares that with 1.*

**除了基本的列类型(整型、浮点型、字符串、日期和布尔)，Hive 也支持数组和映射。**

用户可以从任何原语、集合和其他用户定义类型以编程方式组合自己的类型。

类型系统与 SerDe (序列化/反序列化)和对象检查器接口紧密相连。

用户也可以**通过实现对象检查器，创建类型**。并且**通过使用这些对象检查器，用户可以创建自己的 SerDes** 来序列化和反序列化数据到 HDFS 文件中。这两个接口提供了必要的 hooks ，以便在理解其他数据格式和更丰富的类型时扩展 Hive 的功能。

内置的对象检查器，如 ListObjectInspector、StructObjectInspector 和 MapObjectInspector，提供了必要的原语，以可扩展的方式组合更丰富的类型。

**对于映射和数组，提供了一些有用的内置函数，如 size 和 index 操作符。**

**点符号用于导航嵌套类型，例如 `a.b.c = 1` ，用来查看类型a的字段b的字段c并将其与1进行比较。**

## 3、Metastore

### 3.1、Motivation

*The Metastore provides two important but often overlooked features of a data warehouse: data abstraction and data discovery. Without the data abstractions provided in Hive, a user has to provide information about data formats, extractors and loaders along with the query. In Hive, this information is given during table creation and reused every time the table is referenced. This is very similar to the traditional warehousing systems. The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse. Other tools can be built using this metadata to expose and possibly enhance the information about the data and its availability. Hive accomplishes both of these features by providing a metadata repository that is tightly integrated with the Hive query processing system so that data and metadata are in sync.*

Metastore 提供了数据仓库的两个重要但经常被忽视的特性:数据抽象和数据发现。

**如果在 Hive 中没有提供的数据抽象，用户必须在查询时提供关于数据格式、提取器和加载器的信息**。在 Hive 中，该信息在表创建期间提供，并且在每次引用表时重用。这与传统的仓储系统非常相似。

**数据发现使用户能够发现和探索仓库中的相关和特定数据。还可以使用此元数据构建其他工具，**以公开并增强有关数据及其可用性的信息。

Hive 通过提供与 Hive 查询处理系统紧密集成的元数据存储库来实现这两个特性，这样数据和元数据就可以同步。

### 3.2、Metadata Objects

*Database – is a namespace for tables. It can be used as an administrative unit in the future. The database 'default' is used for tables with no user-supplied database name.*

数据库：所有表的命名空间。默认的数据库是 `default`。

*Table – Metadata for a table contains list of columns, owner, storage and SerDe information. It can also contain any user-supplied key and value data. Storage information includes location of the underlying data, file inout and output formats and bucketing information. SerDe metadata includes the implementation class of serializer and deserializer and any supporting information required by the implementation. All of this information can be provided during creation of the table.*

- 表：表的元数据包含了列组成的列表、所有者、存储和 SerDe 信息。

	它可以包含任意用户提供的 key 和 value 数据。

	存储信息包含了底层数据的位置、文件的输入输出格式和通信息。

	SerDe 元数据包含了序列化器和反序列化器的实现类、和实现类所需的任何支持信息。

	这些信息在创建表时提供。

*Partition – Each partition can have its own columns and SerDe and storage information. This facilitates schema changes without affecting older partitions.*

- 分区：每个分区可以有它自己的列、SerDe 和存储信息。这有助于在不影响旧分区的情况下更改 schema 。

### 3.3、Metastore Architecture

*Metastore is an object store with a database or file backed store. The database backed store is implemented using an object-relational mapping (ORM) solution called the [DataNucleus](https://www.datanucleus.org/). The prime motivation for storing this in a relational database is queriability of metadata. Some disadvantages of using a separate data store for metadata instead of using HDFS are synchronization and scalability issues. Additionally there is no clear way to implement an object store on top of HDFS due to lack of random updates to files. This, coupled with the advantages of queriability of a relational store, made our approach a sensible one.*

**Metastore 是一个对象存储，可以存储在数据库和文件中**。通过使用称为 DataNucleus 的 ORM 实现数据库存储。

存储在关系数据库的主要原因就是**元数据的可查询性**。

使用独立的数据存储，而不是 HDFS 的一个缺点就是**同步和可扩展性问题**。

此外，由于缺少对文件的随机更新，在 HDFS 上没有明确的方法实现对象存储。这一点，再加上关系存储的可查询性优势，使我们的方法更加合理。

*The metastore can be configured to be used in a couple of ways: remote and embedded. In remote mode, the metastore is a [Thrift](https://thrift.apache.org/) service. This mode is useful for non-Java clients. In embedded mode, the Hive client directly connects to an underlying metastore using JDBC. This mode is useful because it avoids another system that needs to be maintained and monitored. Both of these modes can co-exist. (Update: Local metastore is a third possibility. See [Hive Metastore Administration](https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin) for details.)*

**可以使用远程和嵌入式两种方式来配置 metastore** 。

在远程模式下，metastore 是一种 Thrift 服务。此模式对非java客户端很有用。

在嵌入式模式下，Hive 客户端使用 JDBC 直接连接到一个底层的 metastore 。这种模式非常有用，因为它不需要维护和监视的其他系统。

这两种模式都可以共存。

### 3.4、Metastore Interface

*Metastore provides a [Thrift interface](https://thrift.apache.org/docs/idl) to manipulate and query Hive metadata. Thrift provides bindings in many popular languages. Third party tools can use this interface to integrate Hive metadata into other business metadata repositories.*

**Metastore 提供了一个 Thrift 接口，来操作和查询 Hive 元数据**。Thrift 提供了许多流行语言的绑定。第三方工具可以使用此接口将 Hive 元数据集成到其他业务元数据存储库中。

## 4、Hive Query Language

*HiveQL is an SQL-like query language for Hive. It mostly mimics SQL syntax for creation of tables, loading data into tables and querying the tables. HiveQL also allows users to embed their custom map-reduce scripts. These scripts can be written in any language using a simple row-based streaming interface – read rows from standard input and write out rows to standard output. This flexibility comes at a cost of a performance hit caused by converting rows from and to strings. However, we have seen that users do not mind this given that they can implement their scripts in the language of their choice. Another feature unique to HiveQL is multi-table insert. In this construct, users can perform multiple queries on the same input data using a single HiveQL query. Hive optimizes these queries to share the scan of the input data, thus increasing the throughput of these queries several orders of magnitude. We omit more details due to lack of space. For a more complete description of the HiveQL language see the language manual.*

HiveQL 是一种用于 Hive 的类似 SQL 的查询语言。它**模仿了大部分的 SQL 语法，如创建表、往表中载入数据和查询表。**

HiveQL 也**支持用户自己集成自定义的 map-reduce 脚本。**

这些脚本可以用任何语言编写，通过使用一个简单的基于行的流接口--从标准输入读取行数据，再写入到标准输出。

这种灵活性的代价是，在行与字符串之间进行转换会导致性能下降。但是，我们发现用户并不介意这样做，因为他们可以用自己选择的语言来实现脚本。

HiveQL 的另一个独特特性是**多表插入**。在这个结构中，用户可以使用一个 HiveQL 查询语句对同一个输入数据执行多个查询。

Hive 优化了这些查询，以共享输入数据，从而将这些查询的吞吐量提高了几个数量级。

## 5、Compiler

*Parser – Transform a query string to a parse tree representation.*

- Parser：把一个查询字符串转换成一个解析树。

*Semantic Analyser – Transform the parse tree to an internal query representation, which is still block based and not an operator tree. As part of this step, the column names are verified and expansions like * are performed. Type-checking and any implicit type conversions are also performed at this stage. If the table under consideration is a partitioned table, which is the common scenario, all the expressions for that table are collected so that they can be later used to prune the partitions which are not needed. If the query has specified sampling, that is also collected to be used later on.*

- Semantic Analyser：将解析树转换为内部查询表示，它仍然是基于块的，而不是操作符树。

	作为这个步骤的一部分，还会验证列名，执行像 `*`这样的扩展。

	此阶段还将执行类型检查和任何隐式类型转换。

	如果所考虑的表是分区表(这是常见的场景)，则收集该表的所有表达式，以便稍后使用它们来删除不需要的分区。

	如果查询指定了抽样，也将收集该抽样以供以后使用。

*Logical Plan Generator – Convert the internal query representation to a logical plan, which consists of a tree of operators. Some of the operators are relational algebra operators like 'filter', 'join' etc. But some of the operators are Hive specific and are used later on to convert this plan into a series of map-reduce jobs. One such operator is a reduceSink operator which occurs at the map-reduce boundary. This step also includes the optimizer to transform the plan to improve performance – some of those transformations include: converting a series of joins into a single multi-way join, performing a map-side partial aggregation for a group-by, performing a group-by in 2 stages to avoid the scenario when a single reducer can become a bottleneck in presence of skewed data for the grouping key. Each operator comprises a descriptor which is a serializable object.*

- Logical Plan Generator：将内部查询表示形式转换为逻辑计划，其中包含一个操作符树。

	一些运算符是关系代数运算符，如"filter"、"join"等。但也有一些是 Hive 专属。

	之后，运算符将这个计划转换成一系列的 map-reduce jobs 。reduceSink 运算符出现在 map-reduce 边界上。

	这个步骤也会对计划进行优化，以提升性能。如，把一系列 join 转换成一个单独的多向 join；为 group-by 执行 map 端的部分聚合；当分组 key 之后，出现了数据倾斜，一个 reducer 就会成为瓶颈，此时执行一个 group-by 成两个 stages，可避免上述情况。

	每个操作符都包含一个描述符，它是一个可序列化的对象。

*Query Plan Generator – Convert the logical plan to a series of map-reduce tasks. The operator tree is recursively traversed, to be broken up into a series of map-reduce serializable tasks which can be submitted later on to the map-reduce framework for the Hadoop distributed file system. The reduceSink operator is the map-reduce boundary, whose descriptor contains the reduction keys. The reduction keys in the reduceSink descriptor are used as the reduction keys in the map-reduce boundary. The plan consists of the required samples/partitions if the query specified so. The plan is serialized and written to a file.*

- Query Plan Generator：将逻辑计划转换为一系列 map-reduce 任务。

	操作符树被递归遍历，然后分解为一系列 map-reduce 可序列化的任务，这些任务稍后可以提交给 Hadoop 分布式文件系统的 map-reduce 框架。

	reduceSink 操作符是 map-reduce 边界，它的描述符包含 reduction keys。

	reduceSink 描述符中的 reduction keys 用作 map-reduce 边界中的 reduction keys 。

	如果查询指定了这样，则该计划由所需的 samples/partitions 组成。

	执行计划被序列化并写入文件。


## 6、Optimizer

*More plan transformations are performed by the optimizer. The optimizer is an evolving component. As of 2011, it was rule-based and performed the following: column pruning and predicate pushdown. However, the infrastructure was in place, and there was work under progress to include other optimizations like map-side join. (Hive 0.11 added several [join optimizations](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization).)*

优化器执行更多的计划转换。优化器是一个不断发展的组件。从 2011 年开始，它是基于规则的，并执行以下操作:列修剪和谓词下推。然而，基础设施已经到位，并且还在进行工作，以包括其他优化，如 map 端的 join。

*The optimizer can be enhanced to be cost-based (see [Cost-based optimization in Hive](https://cwiki.apache.org/confluence/display/Hive/Cost-based+optimization+in+Hive) and [HIVE-5775](https://issues.apache.org/jira/browse/HIVE-5775)). The sorted nature of output tables can also be preserved and used later on to generate better plans. The query can be performed on a small sample of data to guess the data distribution, which can be used to generate a better plan.*

优化器可以被增强为基于成本的。

还可以保留输出表的排序特性，并在以后用于生成更好的计划。

可以对一个小样本数据执行查询，以猜测数据分布，从而生成更好的计划。

*A [correlation optimizer](https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer) was added in Hive 0.12.*

*The plan is a generic operator tree, and can be easily manipulated.*

## 7、Hive APIs

*[Hive APIs Overview](https://cwiki.apache.org/confluence/display/Hive/Hive+APIs+Overview) describes various public-facing APIs that Hive provides.*