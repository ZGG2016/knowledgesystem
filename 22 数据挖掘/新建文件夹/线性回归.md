#	线性回归

## 多变量线性回归

（具体见视频）

 (梯度下降)

前面介绍的模型只含一个特征。当增加更多的特征时，

![](https://i.imgur.com/szieMnt.png)

假设函数成为了：

![](https://i.imgur.com/DEB48hA.png)

转换成矩阵形式：

![](https://i.imgur.com/xNunC4K.png)

代价函数：

![](https://i.imgur.com/8Ts6zcP.png)

梯度下降公式：

![](https://i.imgur.com/YX5NQi2.png)


## 最小二乘法

	https://blog.csdn.net/y990041769/article/details/69567838

单变量线性回归：对代价函数，分别对参数θ0和θ1求偏导，使其等于0，求得θ0和θ1的值。

	https://blog.csdn.net/wangyangzhizhou/article/details/60133958

多变量线性回归：

最小二乘法可以将误差方程转化为有确定解的代数方程组（其方程式数目正好等于未知数的个数），从而可求解出这些未知参数。这个有确定解的代数方程组称为最小二乘法估计的正规方程（或称为法方程）。

![](https://i.imgur.com/w9SKezm.png)

![](https://i.imgur.com/4iQ9W3y.png)

正规方程的推导：

	https://blog.csdn.net/guang_mang/article/details/77994988
	
## 特征缩放

使特征都具有相近的尺度，一般是[-1,1]，均值归一化

![](https://i.imgur.com/qFxWrOB.png)

另一种方法min-max标准化：

![](https://img-blog.csdn.net/20180123224826146)

## sklearn

**sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)**

参数：

fit_intercept:是否存在截距，默认存在。(e.g. data is expected to be already centered 数据中心化：是指变量减去它的均值。 )

normalize:如果不存在截距，忽略此参数。如果设为true,那么首先对回归因子X减均值，除2范数进行规范化。如果想进行标准化，调用sklearn.preprocessing.StandardScaler，normalize设为false。

LinearRegression将方程分为两个部分存放，coef_存放回归系数，intercept_则存放截距，因此要查看方程，就是查看这两个变量的取值。

方法：

fit(X,y,sample_weight=None)：X,y以矩阵的方式传入，而sample_weight则是每条测试数据的权重，同样以array格式传入。

predict(X)：预测方法，将返回预测值y_pred

score(X,y,sample_weight=None)：评分函数，将返回一个小于1的得分，可能会小于0






 
 
