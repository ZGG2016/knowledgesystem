# 面试准备之大数据其他


## 请说明什么是Apache Kafka? 

	Kafka是一种高吞吐量的分布式发布订阅消息系统。
	kafka作为一个集群运行在一个或多个服务器上。
	kafka集群存储的消息是以topic为类别记录的。
	每个消息是由一个key，一个value和时间戳构成。主要用于处理活跃的流式数据。

## 请说明什么是传统的消息传递方法

	（1）队列：一组用户可以从服务器中读取消息，每条消息都发送给其中一个人。
	
	（2）发布订阅：消息广播给所有的消费者

## 请说明Kafka相对传统技术有什么优势?

	（1）快速:单一的Kafka代理可以处理成千上万的客户端，每秒处理数兆字节的读写操作。
	
	（2）可伸缩:在一组机器上对数据进行分区和简化，以支持更大的数据。
	
	（3）持久:消息是持久性的，并在集群中进行复制，以防止数据丢失。
	
	（4）设计:它提供了容错保证和持久性。

## 在Kafka中broker的意义是什么?

	已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker)。

## Kafka服务器能接收到的最大信息是多少?
	
	100 0000字节

## Kafka的Zookeeper是什么?我们可以在没有Zookeeper的情况下使用Kafka吗?

	（1）ZooKeeper是一个分布式的，开放源码的应用程序协调服务。
	（2）不能。
		Zookeeper主要用于在集群中不同节点之间进行通信；
		在Kafka中，它被用于提交偏移量，因此如果节点在任何情况下都失败了，它都可以从之前提交的偏移量中获取；
		除此之外，它还执行其他活动，如: leader检测、分布式同步、配置管理、识别新节点何时离开或连接、集群、节点实时状态等等。

## Kafka特点

	（1）为发布和订阅提供高吞吐量；
	（2）可进行持久化操作。将消息持久化到磁盘，因此可用于批量消费；
	（3）分布式系统，易于向外扩展。所有的producer、broker和consumer都会有多个，均为分布式的。无需停机即可扩展机器。
	（4）消息被处理的状态是在consumer端维护，而不是由server端维护。当失败时能自动平衡。

## kafka保证消息的顺序？

	kafka通过并行topic的parition来提供了顺序保证。每一个分区都是一个顺序的、不可变的消息队列， 并且可以持续的添加。分区中的消息都被分了一个序列号，称之为偏移量(offset)。消费者消费消息，偏移量也会随着增加。

![](https://i.imgur.com/j9XJyMO.jpg)

## Kafka的副本机制

	由于Producer和Consumer都只会与Leader角色的分区副本相连，
	所以kafka需要以集群的组织形式提供主题下的消息高可用。
	kafka支持主备复制，所以消息具备高可用和持久性。

    一个分区可以有多个副本，这些副本保存在不同的broker上。
	每个分区的副本中都会有一个作为Leader。
	当一个broker失败时，Leader在这台broker上的分区都会变得不可用，kafka会自动移除Leader，再其他副本中选一个作为新的Leader。

	在通常情况下，增加分区可以提供kafka集群的吞吐量。
	然而，也应该意识到集群的总分区数或是单台服务器上的分区数过多，会增加不可用及延迟的风险

	https://www.jianshu.com/p/7833e958fd0c
	https://blog.csdn.net/qq_31598113/article/details/70893878

## kafka消息是否会丢失

	Kafka消息发送分同步(sync)、异步(async)两种方式。默认是使用同步方式。
	Kafka保证消息被安全生产，有三个选项分别是0,1,-1。0代表：不进行消息接收是否成功的确认
	(默认值)；1代表：当Leader副本接收成功后，返回接收成功确认信息；
	-1代表：当Leader和Follower副本都接收成功后，返回接收成功确认信息；


	（1）acks=0，不和Kafka集群进行消息接收确认，则当网络异常、缓冲区满了等情况时，消息可能丢失；
	（2）acks=1、同步模式下，只有Leader确认接收成功后但挂掉了，副本没有同步，数据可能丢失；

	解决办法：

        针对消息丢失：同步模式下，确认机制设置为-1，即让消息写入Leader和Follower之后再确认消息发送成功；异步模式下，为防止缓冲区满，可以在配置文件设置不限制阻塞超时时间，当缓冲区满时让生产者一直处于阻塞状态；

## kafka的leader选举机制是什么？


	Kafka将每个Topic进行分区Patition，以提高消息的并行处理，同时为保证高可用性，
	每个分区都有一定数量的副本 Replica，这样当部分服务器不可用时副本所在服务器就
	可以接替上来，保证系统可用性。在Leader上负责读写，Follower负责数据的同步。
	当一个Leader发生故障如何从Follower中选择新Leader呢？

    Kafka在Zookeeper上针对每个Topic都维护了一个ISR（in-sync replica---已同步的副本）
	的集合，集合的增减Kafka都会更新该记录。如果某分区的Leader不可用，Kafka就从ISR集合中
	选择一个副本作为新的Leader。这样就可以容忍的失败数比较高，假如某Topic有N+1个副本，则可以容忍N个服务器不可用。

    如果ISR中副本都不可用，有两种处理方法：

		（1）等待ISR集合中副本复活后选择一个可用的副本；
		
		（2）选择集群中其他可用副本；


## kafka的消费者如何消费数据

分区中的消息都被分了一个序列号，称之为偏移量(offset)。消费者所持有的仅有的元数据就是这个偏移量，也就是消费者在这个log中的位置。 一个partition仅由同一个消费者组中的一个消费者消费到，并确保消费者是该partition的唯一消费者，并按顺序消费数据。

kafka消费者通过向broker的leader分区发起“提取”(fetch)请求。消费者指定每次请求日志的偏移量并收到那一块日志的起始位置。(生产者推消息到broker，消费者从broker拉取消息)

## kafka：pull模式和push模式的区别优劣

push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成消费者来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。

基于push模式必须选择要么立即发送请求或者积累更多的数据，然后在不知道下游消费者是否能够立即处理它的情况下发送，如果是低延迟，这将导致一次只发送一条消息，以便传输缓存，这是实在是一种浪费，基于pull的设计解决这个问题，消费者总是pull在日志的当前位置之后pull所有可用的消息（或配置一些大size），所以消费者可设置消费多大的量，也不会引起不必要的等待时间。

基于pull模式不足之处在于，如果broker没有数据，消费者会轮询，忙等待数据直到数据到达，为了避免这种情况，我们允许消费者在pull请求时候使用“long poll”进行阻塞，直到数据到达（并且设置等待时间的好处是可以积累消息，组成大数据块一并发送）。

## kafka消费者负载均衡的策略

## kafka文件存储基本结构

	在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。
	
	每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。默认保留7天的数据。 
	
	https://blog.csdn.net/qq_37334135/article/details/78598289 


## kafka存储策略 ##

		1.kafka使用topic管理消息，每个topic包含多个分区，每个分区又对应一个逻辑Log，这个逻辑log由多个
			segment组成。
		2.每个segment存储多条消息，消息id由其逻辑位置(偏移量)决定，从消息id可直接定位到消息的存储位置。
		3.同时每个segment在内存中对应一个index，记录每个segment中的第一条消息的偏移量。
		4.当生产者发布到topic的消息被均与分布到多个分区(或自定义规则)，broker收到消息，往对应的分区的最后一个segment
		  上添加该消息。当某个segment上的消息条数达到配置值或消息发布时间超过阈值时，segment上的消息会被flush到磁盘，
		  只有flush到磁盘上的消息订阅者才能订阅到，segment达到一定的大小后将不会再往该segment写数据，broker会创建新的segment。


## Hbase 的特性

	1.面向列：Hbase是面向列的存储和权限控制，并支持独立索引。列式存储，其数据在表中是按照某列存储的，这样在查询只需要少数几个字段时，能大大减少读取的数据量。
	2.多版本：Hbase每一个列的存储有多个Version。
	3.稀疏性：为空的列不占用存储空间，表可以设计得非常稀疏。
	4.扩展性：底层依赖HDFS。
	5.高可靠性：WAL机制保证了数据写入时不会因集群异常而导致写入数据丢失，Replication机制保证了在集群出现严重的问题时，数据不会发生丢失或损坏。而且Hbase底层使用HDFS，HDFS本身也有备份。
	6.高性能：底层的LSM数据结构和Rowkey有序排列等架构上的独特设计，使得Hbase具有非常高的写入性能。region切分，主键索引和缓存机制使得Hbase在海量数据下具备一定的随机读取性能，该性能真对Rowkey的查询能到达到毫秒级别。

## rowKey的设计原则.

	hbase数据库对行键的设计要求是什么？
	（1）长度原则
	Rowkey 是一个二进制码流，Rowkey 的长度越短越好，不要超过16 个字节。
	（2）散列原则
	将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段。这样将提高数据均衡分布在每个Regionserver 实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个 RegionServer 上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。
	（3）Rowkey唯一原则
	必须在设计上保证其唯一性。

## 订单表如何设计rowkey ##

	用户名、订单号、交易完成时间作为rowkey，具体看业务场景。

## 怎么去设计columnFamily

	把经常一起访问的比较类似的列放在同一个Column Family中；
	columnfamily尽量少，原因是过多的columnfamily之间会互相影响。

## hbase  建一个table

	create 'table name(表名)','column family(列族名)'

## Redis,传统数据库,hbase,hive 每个之间的区别

	Redis是基于内存的key-value数据库，可以持久化到磁盘，所有操作都是原子性的。
	hbase是列式数据库，可以快速的查阅，无法创建主键，存储半结构化或非结构化数据，基于HDFS的。
	hive是数据仓库，存储架构在一个数据库中并处理数据到HDFS,提供SQL类型语言查询HiveQL或HQL。
	传统数据库：存储结构化数据，
	
========================================================================================

## kafka集群的规模，消费速度是多少。

	一般中小型公司是10个节点，每秒20M左右。

## 了解zookeeper吗？介绍一下它，它的选举机制和集群的搭建。 ##

	ZooKeeper是一个开源的分布式协调服务，基于zookeerp可以实现数据的发布订阅、master选举、集群管理、负载均衡、	
	命名服务、分布式协调/通知、配置维护，名字服务、分布式同步、分布式锁和分布式队列等功能。

	选举机制：
		https://blog.csdn.net/liyungfeijob/article/details/79596324
		Leader选举是保证分布式数据一致性的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举。
		
	针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下
　　　　 优先检查ZXID。ZXID比较大的服务器优先作为Leader。
　　　　 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。

	(1) 服务器初始化启动。
		1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,
			所以它的选举状态一直是LOOKING状态
		2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,
			由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到
			超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务
			器1,2还是继续保持LOOKING状态.
		3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与
			上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader.
		4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,
			但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.
		5) 服务器5启动,同4一样,当小弟.
	(2) 服务器运行期间无法和Leader保持连接。
		1) 变更状态。Leader挂后，余下的非Observer服务器都会讲自己的服务器状态变更
		   为LOOKING，然后开始进入Leader选举过程。
　　		2) 每个Server会发出一个投票。在运行期间，每个服务器上的ZXID可能不同，此时假
		   定Server1的ZXID为123，Server3的ZXID为122；在第一轮投票中，Server1和Server3
		   都会投自己，产生投票(1, 123)，(3, 122)，然后各自将投票发送给集群中所有机器。
	https://blog.csdn.net/gaoshan12345678910/article/details/67638657	
	https://blog.csdn.net/zhoulixin8/article/details/51751567


	集群搭建：
		1.搭建java环境
		2.下载解压zookeeper的tar包，配置环境变量
		3.配置zoo.cfg文件（数据存储目录、端口，集群ip）
		4.找到Zookeeper目录，新建data文件夹，并且在data文件夹下面创建一个文件，叫myid，并且在文件里写入server.X对应的X
		5.zkServer.sh start/status

## mysql，mongodb，rides的端口 ##

	mysql：3306，mongdb：27017，rides：6379

## hive:生产环境中为什么建议使用外部表？  ##

	1、因为外部表不会加载数据到hive，减少数据传输、数据还能共享。 
	2、hive不会修改数据，所以无需担心数据的损坏 
	3、删除表时，只删除表结构、不删除数据。 

	外部表:使用external关键字创建的表；
		  外部表对应的文件存储在location指定的目录下,向该目录添加新文件的同时，该表也会读取到该文件(当然文件格式必须跟表定义的一致)；
		  删除外部表的同时并不会删除location指定目录下的文件.

	外部表和内部表的区别：
		1.定义
		2.外部表的数据由hdfs管理，内部表是由hive管理
		3.外部表的数据存储位置由自己决定(location)，内部表则是在hive.metastore.warehouse.dir（默认：/user/hive/warehouse）
		4.删除数据：外部表只会删元数据，HDFS上的文件并不会被删除；内部表会删除元数据和存储数据
		5.修改数据：外部表修改后需要修复(msk repair table tablename)；内部表自动同步到元数据
	https://blog.csdn.net/qq_36743482/article/details/78393678
	https://blog.csdn.net/wawa8899/article/details/80329319

## 你们数据库怎么导入hive的,有没有出现问题  ##

	从hbase或者hdfs导入hive时，数据量太大，会造成exeutors的内存不足，需采用 distribute by 参数
	distribute by + “表中字段” 关键字控制map输出结果的分发,相同字段的map输出会发到一个reduce节点去处理。
	https://blog.csdn.net/chenjieit619/article/details/59480618

	在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。 
	由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。 
	同时，为了防止CLOB字段产生一些问题，因此将HIVE中CLOB字段禁用，禁用的方式如下： 
	[Hadoop@master sqoop-1.4.5]cdcdSQOOP_HOME/conf 
	[hadoop@master conf]$ vi oraoop-site.xml 
	将以下属性的注释去掉，并且将value改为true 
	
	oraoop.import.omit.lobs.and.long 
	true 
	If true, OraOop will omit BLOB, CLOB, NCLOB and LONG columns during an Import. 
	
	
	有些表中虽然有clob字段，但是不能排除掉，因为其他字段使我们所需要，因此在导入的时候采用指定–columns的方式来进行导入 
	sqoop import –hive-import –hive-database test –create-hive-table –connect jdbc –username user–password user 
	–bindir //scratch –outdir /Java –table aaa –columns “ID,NAME” -m 1 –null-string ‘\N’ –null-non-string ‘\N’ 	

## 公司技术选型可能利用storm 进行实时计算,讲解一下storm  ##

	1.Storm是一个免费开源、分布式实时计算系统。
	
	2.Storm采用主从架构，主节点为nimbus负责分发代码、分配任务和监控集群；从节点为supervisor负责启动worker进程执行任务，
	  管理worker进程（每个worker进程包含多个Executor线程，每个Task对应一个execution的线程）。
	  集群的元数据保存在zookeeper中协调主从节点。
	3.在storm中编写的逻辑称为拓扑。拓扑是由spout和bolt组成。spout负责从数据源读取数据，bolt负责逻辑处理。
	（每个 Spout 或者 Bolt 都以跨集群的多个 Task 方式执行. 每个 Task 对应一个 execution 的线程, stream groupings 定义如何从一个 Task 发送 Tuple 到另一个 Task. 可以在 TopologyBuilder 的setSpout 和 setBolt 方法中为每个 Spout 或者 Bolt 设置并行度,）
	
##  Hbase 的特性,以及你怎么去设计 rowkey 和 columnFamily ,怎么去建一个table  ##	

	特性：
		hbase是建立在Hadoop文件系统之上的分布式面向列的数据库

			1.面向列
			2.可扩展：底层依赖HDFS。
			3.稀疏性：为空的列不占用存储空间，表可以设计得非常稀疏。
			4.多版本:每个 cell 都保存着同一份数据的多个版本。版本通过时间戳来索引。
			5.提供灵活的schema模型，因此column被添加到表时不需要预定义。只有table和列簇需要预定义。
			6.不支持复杂的事务(包含：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）以及持久性（Durability）)
					https://blog.csdn.net/noreaday/article/details/81253341
	设计columnFamily:每个列族存放在不同的文件中，建表时列族越少越好
	
	建表：create ‘<table name>’,’<column family>’ 
			create test cf1,cf2

## Redis,传统数据库,hbase,hive 每个之间的区别  ##

	redis：将数据储存在内存里面，读写数据的时候都不会受到硬盘 I/O 速度的限制，所以速度极快。
	传统数据库：存储的数据量有限(GB、TB)、支持事务性、存储结构化数据
	hbase：基于列的非关系型数据库，用于存储海量数据(pb)，底层基于hdfs ，可实现实时查询、不支持事务性、结构半结构
	hive：数据仓库工具，执行的是mapreduce，不适合实时查询、结构半结构。

## storm 如果碰上了复杂逻辑,需要算很长的时间,你怎么去优化,怎么保证实时性  ##

## hive UDF ##

	UDF操作作用于单个数据行，并且产生一个数据行作为输出。大多数函数都属于这一类（比如数学函数和字符串函数）。
		继承UDF类，重写evaluate方法

		java写好的UDF函数编译后的Java类打包成为一个JAR文件，并在Hive中注册这个文件（相当于告诉Hive这个是我写的UDF）：
		ADD JAR /path/hive-sample.jar;
		//给我们写的UDF中的Strip类起个别名
		CREATE TEMPORARY FUNCTION strip AS 'com.hadoop.hive.Strip';

	UDAF 接受多个输入数据行，并产生一个输出数据行。想COUNT和MAX这样的函数就是聚集函数。
	UDTF 操作作用于单个数据行，并且产生多个数据行-------一个表作为输出

## hive\hadoop版本  ##

	hive-1.2.2	 
	hive-2.3.3 稳定版本
	hive-3.1.0

	hadoop2.x  hadoop2.9稳定版本  hadoop3.1最新

## 设计日志收集分析系统 ## 

日志分布在各个业务系统中，我们需要对当天的日志进行实时汇总统计，同时又能按天查询历史的汇总数据（可以围绕PV、UV、IP等 
指标进行阐述）
 
	1、通过flume将不同系统的日志收集到kafka中 
	2、通过storm实时的处理PV、UV、IP 
	3、通过kafka的consumer将日志生产到hive中。 

	PV（Page View）访问量, 即页面浏览量或点击量;
	UV（Unique Visitor）独立访客，访问网站的一台电脑客户端为一个访客。可以理解成访问某网站的电脑的数量;
	IP（Internet Protocol）独立IP数，即统计不同的IP浏览用户数量。
	
## 如果你来做技术分享，你会选择什么主题，课程安排是什么样的？  ##

	1、离线hadoop技术分享（mapreduce、hive） 
	2、nosql数据库hbase分享 
	3、实时流计算分享 

## Hive语句实现WordCount ##
 
	假设数据存放在hadoop下，路径为：/home/hadoop/worddata里面全是一些单词 
	1、建表 
		create table wc(string word) 
			row format delimited
			fields terminated by "\t"
			stored as textfile
			location '/home/hadoop/worddata';
	2、拆分、分组（group by）统计wordcount 
	  select word,count(*) from (select explode(split(line,",")) as word from wc) group by word; 

## 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，找出a、b文件共同的url？ ##

	1.将数据上传到hdfs
	2.对两个文件分别去重
	3.合并两个文件
	4.wordcount，值为2的为共同的url

## String和StringBuffer的区别，StringBuffer与StringBuilder的区别  ##

	区别：
		1.String：字符串常量，不可改变的对象
			（对String对象的任何改变都不影响到原对象，相关的任何change操作都会生成新的对象）
		2.StringBuffer：字符串变量，可改变的对象，线程安全的
		3.StringBuilder：字符串变量，可改变的对象，线程非安全的

	使用场景：
		不需要频繁的拼接字符串的时候使用String，相反需要经常拼接字符串的时候使用
		StringBuilder。StringBuilder与StringBuffer相比，两个类似，不同的是
		StringBuilder是非线程安全的，适合在单线程的情况下使用。多线程情况也可以使用，不过
		需要手动加同步。StrngBuffer是线程安全的，适合在多线程的情况下使用。StringBuilder
		的效率要StringBuffer高

## 实时数据统计会用到哪些技术，它们各自的应用场景及区别是什么？ ##

	flume：日志收集系统，主要用于系统日志的收集 
	kafka：消息队列，进行消息的缓存和系统的解耦 
	storm：实时计算框架，进行流式的计算。 

	解耦:
		在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个隐含的、
		基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。

https://blog.csdn.net/qq_21383435/article/details/79044571

====================================================================

## flume常用的API怎么配置，flume和kafka以及flume和hdfs ##

	1.定义sources、channels、sinks
	2.分别配置属性
	
	agent.sources = s1                                                                                                                  
	agent.channels = c1                                                                                                                 
	agent.sinks = k1                                                                                                                    
	                                                                                                                                      
	agent.sources.s1.type=exec                                                                                                          
	agent.sources.s1.command=tail -F /usr/local/kafka.log                                                                                
	agent.sources.s1.channels=c1                                                                                                        
	agent.channels.c1.type=memory                                                                                                       
	agent.channels.c1.capacity=10000                                                                                                    
	agent.channels.c1.transactionCapacity=100                                                                                           
	                                                                                                                                      
	#设置Kafka接收器                                                                                                                    
	agent.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink                                                                          
	#设置Kafka的broker地址和端口号                                                                                                      
	agent.sinks.k1.brokerList=master:9092,slaver1:9092,slaver2:9092                                                                                               
	#设置Kafka的Topic                                                                                                                   
	agent.sinks.k1.topic=kafkatest                                                                                                      
	#设置序列化方式                                                                                                                     
	agent.sinks.k1.serializer.class=kafka.serializer.StringEncoder                                                                      
	                                                                                                                                      
	agent.sinks.k1.channel=c1

	=====================================
	Kafka-source:
		a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
		a1.sources.r1.kafka.bootstrap.servers = hadoop:9092
		a1.sources.r1.kafka.topics = test

	Kafka-sink:
		a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
		a1.sinks.k1.kafka.topic = mytopic
		a1.sinks.k1.kafka.bootstrap.servers = localhost:9092 
	=====================================
	HDFS-sink
		a1.sinks.k1.type = hdfs
		a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S
		a1.sinks.k1.hdfs.filePrefix = events-
					hdfs.fileType  SequenceFile
					hdfs.maxOpenFiles 5000
					hdfs.minBlockReplicas 
					hdfs.writeFormat	Writable

## HBase是怎么设计的？rowKey怎么设计 ##

	1.定义：HBase是一个基于列的分布式数据库。需要运行在 HDFS 之上,HBase 上层提供了访问的数据的 Java API 层
	2.表：hbase的表由多个列族组成，列族由列组成。数据根据rowkey排序，按row-key排序的数据进行水平切分，每一片称为一个Region
	3.架构：主要包括master、region server和zoopkeeper三个模块。
				master:分配region、管理监控regionserver，创建删除表操作；
				region server:执行读写操作，客户端访问数据，直接与region server通信。
				zookeeper：协调集群，维护集群状态（HBase Master的HA解决方案）。	
	https://www.cnblogs.com/steven-note/p/7209398.html
	https://www.cnblogs.com/ajianbeyourself/p/7790044.html
	============================================
	

## Hbase有多少数据，怎么查询的 ##
	
	get 'table name', ‘rowid’, {COLUMN => ‘column family:column name ’}

	Configuration config = HBaseConfiguration.create();

      // Instantiating HTable class
      HTable table = new HTable(config, "emp");

      // Instantiating Get class
      Get g = new Get(Bytes.toBytes("row1"));

      // Reading the data
      Result result = table.get(g);

      // Reading values from Result class object
      byte [] value = result.getValue(Bytes.toBytes("personal"),Bytes.toBytes("name"));

      byte [] value1 = result.getValue(Bytes.toBytes("personal"),Bytes.toBytes("city"));

      // Printing the values
      String name = Bytes.toString(value);
      String city = Bytes.toString(value1);

## 如何设计rowkey，region是怎么回事，怎么划分的，怎么定位的，写***作是怎么样的，什么情况下会刷写，wal是干啥用的，为啥要有这个东西存在，cap理论 ##

一面问drop truncate delete区别
还有throw和throws的区别 
还有b+树索引与hash索引对比
还有几个linux命令
大数据问我sparkstreaming和storm的编程过程
hive hbase的架构和原理
impala实现细节
tez用来做什么的
垃圾回收期有哪些