# 中电健康云面试 #

职位：大数据工程师（实习）

过程：笔试+2轮技术面试+hr面试+cto面试

## 笔试题 ##

### 1.java中方法重写与重载的区别 ###

	方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。

	方法重写(0veriding)：对父类的方法进行重新定义，且子类中的方法与父类中继承的
						方法有完全相同的返回值类型、方法名、参数个数以及参数类型。
						建立在继承关系上；
	方法重载(Overloading)：在一个类中，多个方法的方法名相同，但是参数列表不同。
						  参数列表不同指的是参数个数、参数类型或者参数的顺序不同。
						  **只能通过参数列表，重载的方法的返回值类型可以相同也可以不相同**

	1.如果子类将父类中的方法重写了，调用的时候肯定是调用被重写过的方法，那么如果现在一定要调用父类中的方法，使用super关键就可以实现。
	2.被子类重写的方法不能拥有比父类方法更加严格的访问权限。如果定义父类的方法为public，在子类定义为private，程序运行时就会报错。
	3.当父类中方法的访问权限修饰符为private时，在子类是不能被重写的。
	4.在继承过程中如果父类当中的方法抛出异常，那么在子类中重写父类的该方法时，也要抛出异常，
	  而且抛出的异常不能多于父类中抛出的异常(可以等于父类中抛出的异常)。换句话说，重写方法一定不能抛出新的检查异常，
      或者比被重写方法声明更加宽泛的检查型异常。例如，父类的一个方法申明了一个检查异常IOException，在重写这个方法时就不能抛出Exception，
      只能抛出IOException的子类异常，
	5.如果子类中创建了一个成员变量，而该变量和父类中的一个变量名称相同，称作变量重写或属性覆盖。

	6.构造方法也可以重载。
	7.不能通过访问权限、返回类型、抛出的异常进行重载。
	8.方法的异常类型和数目不会对重载造成影响。
	9.可以有不同的访问修饰符。
	10.可以抛出不同的异常。

	区别：https://www.cnblogs.com/zheting/p/7751787.html

![](https://i.imgur.com/GIilBK6.jpg)

### 2.列出常见的运行异常 ###

	运行异常：可以编译通过、但在运行出现。

	NullPointException、ClassNotFoundException、ArrayOutOfBoundsException
	ClassCastException、IllegalArgumentException

	异常：https://www.cnblogs.com/skywang12345/p/3544168.html
		 http://www.importnew.com/26613.html

![](https://i.imgur.com/KU1SqKb.png)		

### 3.float f = 4.2,书写是否正确 ###

	float f = 4.2f

### 4.hive和传统数据库的区别 ###

	(1)存储：所有Hive的数据都是存储在 HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中。
	(2)数据格式：Hive 中没有定义专门的数据格式，数据格式可以由用户指定。在数据库中，不同的数据库有不同的存储引擎，定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。（读时模式、写时模式）
	(3)执行:Hive 中大多数查询的执行是通过 Hadoop提供的 MapReduce来实现的（类似 select * from tbl的查询不需要 MapReduce）。而数据库通常有自己的执行引擎。
	https://blog.csdn.net/nisjlvhudy/article/details/47175705

### 5.spark的宽依赖和窄依赖 ###

	宽依赖：父RDD的分区被子RDD的多个分区使用.例如 groupByKey、reduceByKey、sortByKey等操作会产生宽依赖，会产生shuffle

	窄依赖：父RDD的每个分区都只被子RDD的一个分区使用  例如map、filter、union等操作会产生窄依赖

### 6.spark如何处理数据倾斜 ###

	(1)通过Hive来进行数据预处理,即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他
		表进行join。把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。
	(2)如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那
		么干脆就直接过滤掉那少数几个key。
	(3)在对RDD执行shuffle算子时，给shuffle算子传入一个并行度参数。
	(4)先给每个key都打上一个随机数,进行局部聚合，对于局部聚合的结果进行全局聚合操作
	(5)将reduce join转为map join

	https://blog.csdn.net/qq_38534715/article/details/78707759
	http://www.jasongj.com/spark/skew/

	https://www.cnblogs.com/junneyang/p/8267967.html

### 7.用spark写wordcount ###

	sc.flatMap(lambda x:x.split(",")).map(lambda x:(x,1)).reduceByKey(add)

### 8.借助wordcount实例，写出mapreduce的执行流程 ###

![](https://i.imgur.com/hVhpsLR.png)

### 9.一道sql题 ###

	group by \ count \ left join


============================================================================

## 面试题 ##

### 1.用过哪些java设计模式，写线程安全的单例模式 ###

	•创建型模式（5种）：工厂方法模式，抽象工厂模式，单例模式，建造者模式，原型模式。
	•结构型模式（7种）：适配器模式，装饰器模式，代理模式，外观模式，桥接模式，组合模式，
						享元模式。
	•行为型模式（11种）：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、
						命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。

	public class Singleton{
		private Singleton(){}

		private static Singleton instance = new Singleton();

		public static Singleton getInstance(){
			return instance;
		}

	}

### 2.spark如何将数据缓存到内存中

	RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。

### 3.map、mapPartitions的区别

	map的输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区
	mapPartitions性能高但会内容OOM异常

### 4. spark shuffle

	https://www.2cto.com/net/201712/703242.html

### 5. java 多线程

### 6. storm的分组策略，及各自含义

	shufflegrouping、fieldgrouping、allgrouping、nonegrouping
	globalgrouping、directGrouping、Local or shuffle Grouping

	https://www.cnblogs.com/xymqx/p/4365190.html

### 7.storm的ack-fail机制

	Spout 会提供一个 "message id"，用于以后标识 tuple。一个 tuple 从 spout 流出，会形
	成一颗元组树。如果元组树上的元组被完全处理，那么会调用spout的ack方法，同样的，如果处理
	tuple超时了，Storm会调用在 Spout 上调用 fail 方法。

### 8.hive往表中导入数据的方式

	1.load data [local] inpath "path/file.txt" overwrite into table 表名

	(1)从本地导入
			load data local inpath "path/file.txt" into table 表名;
	(2)从hdfs导入
			load data inpath "path/file.txt" into table 表名;

	2.使用hadoop fs -put ... 导入到hive表的路径上
	3.insert into(overwrite) table 表名 select_statement1 FROM from_statement

	================================================
	从Hive表导出数据方式

	1.insert overwrite local directory "path/" row format delimited
		fields terminated by "\t" select * from db_hive_demo.emp ;
	2.insert overwrite  directory 'hdfs://hadoop102:8020/user/hive/warehouse/	emp2/emp2'	select * from emp where empno >7800;
	3.Bash shell覆盖追加导出
         例如：$ bin/hive -e "select * from staff;"  > /home/z/backup.log
	4.Sqoop把hive数据导出到外部

	https://blog.csdn.net/qq_26442553/article/details/79478839

### 9.storm 中 spout和bolt间的通信协议  ????

### 10.解释kafka的lead和follower

	Kafka每个topic的partition有N个副本，其中N是topic的复制因子。N个replicas中。其中一个replica为leader，其他都为follower，leader处理partition的所有读写请求，与此同时，follower会被动定期地去复制leader上的数据。如果leader发生故障或挂掉，一个新leader被选举并接收客户端的消息成功写入。
	https://blog.csdn.net/a953713428/article/details/80072610

### 11.MapReduce中压缩技术的使用

	1.使用压缩数据作为MapReduce的输入时，需要确认数据的压缩格式是否支持切片。Gzip格式是不
	  支持的，LZO、LZ4、Snappy支持。MapReduce读取输入路径中的压缩文件时会自动完成数据解压。

	2.MapReduce Job的结果输出需要使用压缩，可以通过设置Job的相关配置属性来实现：

		mapreduce.output.fileoutputformat.compress：true		
		mapreduce.output.fileoutputformat.compress.codec：CompressionCodec全限定类名

		也可以通过FileOutputFormat提供的静态方法设置，如：

		FileOutputFormat.setCompressOutput(job, true);
		FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);

		不同的输出文件格式可能相应的设置属性会有不同。

	3.对map的输出进行压缩

		Map Task的输出被写出到本地磁盘，而且需要通过网络传输至Reduce Task的节点，只要简
		单地使用一个快速的压缩算法（如LZO、LZ4、Snappy）就可以带来性能的提升，因为压缩机
		制的使用避免了Map Tasks与Reduce Tasks之间大量中间结果数据被传输。可以通过设置相
		应的Job配置属性开启：

		mapreduce.map.output.compress：true

		mapreduce.map.output.compress.codec：CompressionCodec全限定类名

		也可以通过Configuration API进行设置：

		Configuration conf = new Configuration();
		conf.setBoolean(Job.MAP_OUTPUT_COMPRESS, true);
		conf.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class);
		Job job = new Job(conf);

	http://www.cnblogs.com/yurunmiao/p/4528499.html

### 12.mapreduce过程

	读数据划分-->map-->分区-->排序-->(combiner)
		-->copy-->group-->reduce

	从mapper的输出copy过来，进行排序，把key相同的分到一组(合并)，执行一个reduce。

	<1>文件会被划分成多个inputsplit，每一个InputSplit都会分配一个Mapper任务,Mapper任务的输出存放在缓存中，每个map有一个环形内存缓冲区，用于存储任务的输出。默认大小100MB（io.sort.mb属性），一旦达到阀值0.8(io.sort.spill.percent),一个后台线程就把内容写到(spill)Linux本地磁盘中的指定目录（mapred.local.dir）下的新建的一个溢出写文件。

	<2>写磁盘前，要partition,sort。通过分区，将不同类型的数据分开处理，之后对不同分区的数据进行排序，如果有Combiner，还要对排序后的数据进行combine。等最后记录写完，将全部溢出文件合并为一个分区且排序的文件(归并排序)。

	<3>最后将磁盘中的数据送到Reduce中。Reducer通过Http方式得到输出文件的分区，存到内存或磁盘，排序合并。然后走Reduce阶段。

	分组：相同的key的值分到一组。（自定义分组：实现RawComparator接口，再配置job.setGroupingComparatorClass）
	分区：根据key的hash值进行分区。（自定义分区：继承HashPartitioner，重写getPartition，配置job.setPartitionerClass(KpiPartitioner.class);）

	https://www.cnblogs.com/sunddenly/p/4009751.html
	http://blog.sina.com.cn/s/blog_6b1ff7650101imzp.html
	https://www.cnblogs.com/ahu-lichang/p/6645074.html
	https://yq.aliyun.com/articles/43430
	官方文档http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html

### 13.mapReduce有几种排序及排序发生的阶段  (???)

	1.部分排序:map端，MapReduce中默认的排序方式,默认输出是按照键的自然顺序排列。
			通过job.setNumReduceTasks(n)，来改变我们reduce的个数，在n等于2的情况下，那
			么一个Wordcount可能的输出就会是一个reduce内有序，也就是说最后产生的两个结果
			文件，它们只是在内部有序，两个文件之间却不是有序的，所以这也叫部分排序。
	2.全局排序：shuffle过程中。由于一个map中的每个分区内是有序的，然后会merge有两次，一次是map端将多个spill按照分区和分区内的key进行merge，形成一个大的文件。第二次merge是在reduce端，进入同一个reduce的多个map的输出 merge在一起
	3.二次排序：reduce端。首先按照第一字段排序，然后再对第一字段相同的行按照第二字段排序

### 14.mapper阶段会调用几次map函数,map类中重写哪些方法，继承什么类

	每行数据调一个map；（流式读取数据）
	setup、map、cleanup;
	mapper

### 15.spark数据分区

	分区是指如何把RDD分布在spark集群的各个节点的操作；Spark使用分区来管理数据，这些分区
	有助于并行化分布式数据处理，并以最少的网络流量在executors之间发送数据。

	分区并非对于所有场景都是有好处的：比如， 如果给定RDD只被扫描一遍，那么完全没有必要做分区，
	只有当数据多次在诸如连接这种基于键的操作时，分区才会有帮助。

	默认情况下，每个HDFS的分区文件（默认分区文件块大小是128M）都会创建一个RDD分区。

	分区块越小，分区数量就会越多。分区数据就会分布在更多的worker节点上。但分区越多意味着
	处理分区的计算任务越多，太大的分区数量（任务数量）可能是导致Spark任务运行效率低下的原因之一。

	Spark只能为RDD的每个分区运行1个并发任务，直到达到Spark集群的CPU数量。
	所以，如果你有一个拥有50个CPU的Spark集群，那么你可以让RDD至少有50个分区（或者是CPU数量的2到3倍）。

	Repartition、coalesce

	spark.default.parallelise设置要用于HashPartitioner的分区数。它对应于调度程序后端的默认并行度。

	通过函数调用获取分区数量RDD.getNumPartitions   rdd.partitions.size

	实现自定义的分区器，需要继承org.apache.spark.Partitioner类并实现下面三个方法：
		1、numPartitions:Int:返回创建出来的分区数
		2、getPartition():int:返回给定键的分区编号（0到numPartitions-1），确保永远返回的是一个非负数;
		3、equals():java判断相等的标准方法，这个方法的是吸纳非常重要，Spark需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样spark才可以判断两个RDD的分区方式是否相同。

	https://blog.csdn.net/zg_hover/article/details/73476265


### 16.hive 分区

	作用：在Hive Select查询中一般会扫描整个表内容，会消耗很多时间做没必要的工作。
		有时候只需要扫描表中关心的一部分数据，因此建表时引入了partition概念。
	分区表指的是在创建表时指定的partition的分区空间。
	如果需要创建有分区的表，需要在create表的时候调用可选参数partitioned by
	一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下。

	创建分区：
			单分区建表语句：create table day_table (id int, content string) partitioned by (dt string);单分区表，按天分区，在表结构中存在id，content，dt三列。
			双分区建表语句：create table day_hour_table (id int, content string) partitioned by (dt string, hour string);

	数据加载：
			LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
			例：
			LOAD DATA INPATH '/user/pv.txt' INTO TABLE day_hour_table PARTITION(dt='2008-08- 08', hour='08');
			LOAD DATA local INPATH '/user/hua/*' INTO TABLE day_hour partition(dt='2010-07- 07');

	添加分区：
			ALTER TABLE table_name ADD partition_spec [ LOCATION 'location1' ] partition_spec [ LOCATION 'location2' ] ... partition_spec: : PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)
			用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。当分区名是字符串时加引号。例：
			ALTER TABLE day_table ADD PARTITION (dt='2008-08-08', hour='08') location '/path/pv1.txt' PARTITION (dt='2008-08-08', hour='09') location '/path/pv2.txt';

	删除分区语法：
			ALTER TABLE table_name DROP partition_spec, partition_spec,...
			用户可以用 ALTER TABLE DROP PARTITION 来删除分区。分区的元数据和数据将被一并删除。例：
			ALTER TABLE day_hour_table DROP PARTITION (dt='2008-08-08', hour='09');

	修复分区就是重新同步hdfs上的分区信息。
			msck repair table table_name;

	查询分区：
			show partitions table_name;

	https://blog.csdn.net/qq_36743482/article/details/78418343
	https://blog.csdn.net/yidu_fanchen/article/details/77683558
	https://www.cnblogs.com/ilvutm/p/7152499.html

### 17.hive的分桶

	将表或分区组织成桶有以下几个目的：
	　　1.为看取样更高效，因为在处理大规模的数据集时，在开发、测试阶段将所有的数据全部处理一遍可能不太现实，这时取样就必不可少。
			抽样：
				SELECT * FROM table_name TABLESAMPLE(nPERCENT)；
				就是针对n%进行取样
				有了桶之后呢？
				SELECT * FROM film TABLESAMPLE(BUCKET x OUTOF y)
					x：表示从哪一个桶开始抽样
					y：抽样因素，必须是桶数的因子或者倍数,假设桶数是100那么y可以是200,10,20,25,5。
					   假设桶数是100，y=25时抽取(100/25) = 4个bucket数据;当y=200的时候(100/200) = 0.5个bucket的数据
	　　2.为了获得更好的查询处理效率。
			大表在JOIN的时候，效率低下。如果对两个表先分别按id分桶，那么相同id都会归入一个桶。
			那么此时再进行JOIN的时候是按照桶来JOIN的，那么大大减少了JOIN的数量。

	创建分桶： 在建立桶之前，需要设置hive.enforce.bucketing属性为true，使得hive能识别桶。
			CREATE TABLE IF NOT EXISTS t_movie(id INT,name STRING,director STRING,country STRING,year STRING,month STRING
				)ROWFORMAT DELIMITED FIELDS TERMINATED BY ',';
				PARTITIONEDBY (area STRING)
				CLUSTERED BY (country) INTO 4 BUCKETS
				ROWFORMAT DELIMITED FIELDS TERMINATED BY ','
				STORED AS ORC;

	导入数据：
			INSERT INTO TABLE t_movie PARTITION(area='China') SELECT * FROM t_movie WHERE country = 'China';

### 18.rdd是什么

	弹性分布式数据集；

	表示已被分区，不可变的并能够被并行操作的数据集合；

	从两种数据源中创建 RDD 第一是稳定存储中的数据； 第二是其他 RDD，产生了血统，即 RDD 有充足的信息关于它是如何从其他 RDD 产生而来的。

	RDD可以存储任意类型的数据；RDD可以在存储在内存和磁盘之间，并且自动或者手动切换；具有良好的容错性；

	对于RDD可以有两种操作算子：转换（Transformation）与行动（Action）。Transformation操作是延迟计算的，也就是说从一个RDD转换生成
	另一个RDD的转换操作不是马上执行，需要等到有Action操作的时候才会真正触发运算。Action算子会触发Spark提交作业（Job），并将数据输出Spark系统。

### 19.hashmap hashtable区别

	1.HashTable和HashMap采用相同的存储机制，哈希表（数组+链表）。
	2.HashMap是非线程安全的。HashTable是线程安全的，内部的方法基本都经过synchronized修饰。
	3.HashMap可以存储null键和null值，HashTable和ConCurrentHashMap中put进的键值只要有一个null，直接抛出NullPointerException。
	4.HashMap默认初始化数组的大小为16，HashTable为11。前者扩容时乘2，后者为乘2加1。

### 20.arraylist linkedlist

	1.ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。
	2.对于随机访问get和set，ArrayList觉得优于LinkedList，因为LinkedList要移动指针。
	3.对于新增和删除操作add和remove，LinedList比较占优势，因为ArrayList要移动数据。

### 21.对hbase的理解

	建立在Hadoop文件系统之上的分布式面向列的数据库；
	可以直接或通过HBase的存储HDFS数据。使用HBase在HDFS读取消费/随机访问数据；
	在表中它由行排序；一个表有多个列族以及每一个列族可以有任意数量的列。

	HBase是由三种类型的服务器以主从模式构成的。这三种服务器分别是：Region server，HBase HMaster，ZooKeeper。
	其中Region server负责数据的读写服务。用户通过沟通Region server来实现对数据的访问。
	HBase HMaster负责Region的分配及数据库的创建和删除等操作。
	ZooKeeper作为HDFS的一部分，负责维护集群的状态（某台服务器是否在线，服务器之间数据的同步操作及master的选举等）。

	https://blog.csdn.net/Yaokai_AssultMaster/article/details/72877127

### 22.reduceByKey和groupByKey的区别

	reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。

	groupByKey:
		groupByKey也是对每个key进行操作，但只生成一个sequence。如果需要对sequence进行aggregation操作
		（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。
		这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。

	另外，如果仅仅是group处理，那么以下函数应该优先于 groupByKey ：
	　　（1）、combineByKey 组合数据，但是组合之后的数据类型与输入时值的类型不一样。
	　　（2）、foldByKey合并每一个 key 的所有值，在级联函数和“零值”中使用。

	https://blog.csdn.net/zongzhiyuan/article/details/49965021

### 23.flume的三个组件，自定义source

	source/channel/sink

	自定义的消息有两种类型的Source,PollableSource与EventDrivenSource。

	两者的区别在于PollableSource是通过线程不断去调用process方法，主动拉取消息，而EventDrivenSource是需要触发一个调用机制，即被动等待。

	extends AbstractSource implements Configurable, PollableSource

	start()、stop()、process()、configure()、getMaxBackOffSleepInterval()、getBackOffSleepIncrement()

	（自定义flume的sink，用户自定义sink在flume中只需要继承一个基类：AbstractSink，然后实现其中的方法就可以了
		https://blog.csdn.net/harderxin/article/details/75032337）

### 24.kafka 数据倾斜

### 25.kafka分区

	对于每一个Topic，划分为多个分区，每一个分区都是一个顺序的、不可变的消息队列， 并且可以持续的添加。
	分区中的消息都被分了一个序列号，称之为偏移量(offset)，在每个分区中此偏移量都是唯一的。

	每个partition仅由同一个消费者组中的一个消费者消费到。并确保消费者是该partition的唯一消费者，并按顺序消费数据。
	但是它也仅仅是保证Topic的一个分区顺序处理，不能保证跨分区的消息先后处理顺序。

### 26.mapreduce输入相关

	 Hadoop 中的MapReduce库支持几种不同格式的输入数据。例如，文本模式的输入数据的每一行被视为一个key/value pair,
	其中key为文件的偏移量，value为那一行的内容。每一种输入类型的实现都必须能够把输入数据分割成数据片段，并能够由单独的Map任务来对数据片段进行后续处理。

	InputFormat类：
	InputFormat为Hadoop作业的所有输入格式的抽象基类，它描述了作业输入需要满足的规范细节。
	两个方法：getRecordReader和getSplits

	InputSplit类：
	在HDFS中，当文件大小少于HDFS的块容量时，每个文件将创建一个InputSplit实例。而对于被分割成多个块的文件，将使用更复杂的公式来计算InputSplit的数量。

	RecordReader类：
	将数据以一条条记录（record）的方式向Map传递。RecordReader在InputSplit类内部执行，并将数据以key-value的形式产生一条条的记录。

	https://blog.csdn.net/e15273/article/details/78529120
	https://www.cnblogs.com/geekszhong/p/4480387.html
