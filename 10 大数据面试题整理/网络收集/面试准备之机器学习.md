# 面试准备之机器学习

## 1、

![](https://i.imgur.com/DDXvJy5.jpg)

	防止过拟合的方法：
		增加样本量；
		提前终止；
		Dropout；
		Bootstrap重采样；
		交叉验证法；
		正则化。
		https://blog.csdn.net/Left_Think/article/details/77684087?locationNum=5&fps=1

	========================================
	Bootstrap重采样：
			给定包含m个样本的数据集D，每次随机的从D中挑选一个样本，将其放入D'中，然后将该样本放回D中，使得该样本在下次抽样中会被抽到，这个过程重复m次后，我们就得到了一个包含m个样本的训练集D'，可使用D\D'作为测试集。

	在数据集小，难以划分训练集和测试集时有效。

	主要利用集成学习来防止过拟合。

## 2、

![](https://i.imgur.com/RtUlhiA.jpg)

	 LDA主题模型（Latent Dirichlet Allocation）：
		https://www.cnblogs.com/pinard/p/6831308.html
		https://www.cnblogs.com/pinard/p/6831308.html


	 线性判别分析（Linear Discriminant Analysis）：
		LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。
		这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。
		LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。
		我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，
		而不同类别的数据的类别中心之间的距离尽可能的大。
		http://www.cnblogs.com/pinard/p/6244265.html

## 3、

![](https://i.imgur.com/ZyvFB7e.jpg)

	无论是生成式模型还是判别式模型，都可作为分类器使用，
	分类器的数学表达即为：给定输入X以及分类变量Y，求 P(Y|X)。

	判别式模型（Discriminative Model）是直接对条件概率p(y|x;θ)建模。常见的判别式模型有 线性回归模型、线性判别分析、支持向量机SVM、神经网络等。

	生成式模型（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi，

	https://blog.csdn.net/lanchunhui/article/details/60321358

## 4、？？？

![](https://i.imgur.com/4DZqrug.jpg)

## 5、

![](https://i.imgur.com/Re1lSyw.jpg)

	precision = TP / (TP + FP)
	recall = TP / (TP + FN)
	accuracy = (TP + TN) / (TP + FP + TN + FN)
	F1 Score = 2*P*R/(P+R)，其中P和R分别为 precision 和 recall
	https://www.cnblogs.com/weedboy/p/7072010.html

## 6、

![](https://i.imgur.com/XJnBJCH.jpg)

	共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，
	但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hes	se矩阵并求逆的缺点。
	其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。

	牛顿法的优缺点总结：
	　　优点：二阶收敛，收敛速度快；
	　　缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂

	梯度下降法的缺点：
	　	靠近极小值时收敛速度减慢，如下图所示；
		直线搜索时可能会产生一些问题；
		可能会“之字形”地下降。

## 7、

![](https://i.imgur.com/6vQxNxi.jpg)

	Principal Component Analysis(PCA)是最常用的线性降维方法，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。
	Linear Discriminant Analysis (也有叫做Fisher Linear Discriminant)是一种有监督的（supervised）线性降维算法。与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分！
	 
	Locally linear embedding（LLE）[1]是一种非线性降维算法，它能够使降维后的数据较好地保持原有流形结构。LLE可以说是流形学习方法最经典的工作之一。很多后续的流形学习、降维方法都与LLE有密切联系。#
	Laplacian Eigenmaps（拉普拉斯特征映射）看问题的角度和LLE有些相似，也是用局部的角度去构建数据之间的关系。它的直观思想是希望相互间有关系的点（在图中相连的点）在降维后的空间中尽可能的靠近。Laplacian Eigenmaps可以反映出数据内在的流形结构。

	http://dataunion.org/13451.html

## 8、

![](https://i.imgur.com/5Un7pY5.jpg)

## 9、

![](https://i.imgur.com/3G4tomY.jpg)

	无向图模型也叫马尔科夫随机场(MarkovRandomFields)或马尔科夫网络(MarkovNetwork)
	有向图模型也叫贝叶斯网络(Bayesiannetworks)或信念网络(BeliefNetworks)

## 10、

![](https://i.imgur.com/KLkRnvl.jpg)


## 11、

![](https://i.imgur.com/097ekWb.jpg)


	神经网络可以用于线性和非线性分类。（一般非线性的模型只要数据量足够大均可用于线性分类任务）
	神经网络每层激活函数不必相同，可根据需要选择。
	激活函数relu，值域范围就不在[-1,1]之间，而在[0,+无穷)

## 12、

![](https://i.imgur.com/F9b63TQ.jpg)

	前提：测试数据不变，只增加训练数据
	1.减少过拟合，泛化更好，测试误差变小
	2.训练集多样性增加，数据输入分布改变，训练误差变大

## 13、

![](https://i.imgur.com/879H5FA.jpg)

## 14、

![](https://i.imgur.com/ihPGmAg.jpg)

## 15、

![](https://i.imgur.com/ZvuZlAK.jpg)

![](https://i.imgur.com/8C9K7zk.jpg)
答案有问题

## 16、

![](https://i.imgur.com/InZybIo.jpg)

## 17、

![](https://i.imgur.com/ngE3gJE.jpg)

## 18、

![](https://i.imgur.com/1QhKMDV.jpg)

## 19、

![](https://i.imgur.com/giLVYT4.jpg)

## 20、

![](https://i.imgur.com/q1Bsrs8.jpg)

![](https://i.imgur.com/HbDVMvb.jpg)

## 21、

![](https://i.imgur.com/itS2dbr.jpg)

## 22、

![](https://i.imgur.com/2M1MnOs.jpg)

## 23、

![](https://i.imgur.com/SpQUAfA.jpg)

	如果:AA'=E(E为单位矩阵,A'表示“矩阵A的转置矩阵”。)或A′A=E,则n阶实矩阵A称为正交矩阵

## 24、

![](https://i.imgur.com/Lp3WFth.jpg)