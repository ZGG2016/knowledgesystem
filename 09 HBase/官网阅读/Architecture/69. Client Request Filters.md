# 69. Client Request Filters

[TOC]

> [Get](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html) and [Scan](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html) instances can be optionally configured with [filters](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/Filter.html) which are applied on the RegionServer.

**Get 和 Scan 实例可以可选地配置过滤器，应用在 RegionServer 上**。

> Filters can be confusing because there are many different types, and it is best to approach them by understanding the groups of Filter functionality.

过滤器有多种不同的类型。最好是通过理解过滤器功能组来接近它们。

## 69.1. Structural

> Structural Filters contain other Filters.

结构过滤器包含其他过滤器。

```java
//帮助理解
/**
 * This is a generic filter to be used to filter by comparison.  It takes an
 * operator (equal, greater, not equal, etc) and a byte [] comparator.
 * <p>
 * To filter by row key, use {@link RowFilter}.
 * <p>
 * To filter by column family, use {@link FamilyFilter}.
 * <p>
 * To filter by column qualifier, use {@link QualifierFilter}.
 * <p>
 * To filter by value, use {@link ValueFilter}.
 * <p>
 * These filters can be wrapped with {@link SkipFilter} and {@link WhileMatchFilter}
 * to add more control.
 * <p>
 * Multiple filters can be combined using {@link FilterList}.
 */
@InterfaceAudience.Public
public abstract class CompareFilter extends FilterBase
```

### 69.1.1. FilterList

> [FilterList](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FilterList.html) represents a list of Filters with a relationship of FilterList.Operator.MUST_PASS_ALL or FilterList.Operator.MUST_PASS_ONE between the Filters. The following example shows an 'or' between two Filters (checking for either 'my value' or 'my other value' on the same attribute).

FilterList 是多个过滤器的列表，**这些过滤器两两间具有`FilterList.Operator.MUST_PASS_ALL`或`FilterList.Operator.MUST_PASS_ONE`关系**。

下面的示例展示了两个过滤器间的 'or' 关系(在同一个属性上检查 'my value' 或 'my other value')。

```java
FilterList list = new FilterList(FilterList.Operator.MUST_PASS_ONE);
SingleColumnValueFilter filter1 = new SingleColumnValueFilter(
  cf,
  column,
  CompareOperator.EQUAL,
  Bytes.toBytes("my value")
  );
list.add(filter1);
SingleColumnValueFilter filter2 = new SingleColumnValueFilter(
  cf,
  column,
  CompareOperator.EQUAL,
  Bytes.toBytes("my other value")
  );
list.add(filter2);
scan.setFilter(list);
```

## 69.2. Column Value

### 69.2.1. SingleColumnValueFilter

> A SingleColumnValueFilter (see: [https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.html](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.html)) can be used to test column values for equivalence (CompareOperaor.EQUAL), inequality (CompareOperaor.NOT_EQUAL), or ranges (e.g., CompareOperaor.GREATER). The following is an example of testing equivalence of a column to a String value "my value"…​

SingleColumnValueFilter 可以用来测试列值是否等于(CompareOperaor.EQUAL)、不等(CompareOperaor.NOT_EQUAL)或范围(如CompareOperaor.GREATER)。

```java
SingleColumnValueFilter filter = new SingleColumnValueFilter(
  cf,
  column,
  CompareOperaor.EQUAL,
  Bytes.toBytes("my value")
  );
scan.setFilter(filter);
```

```java
// 枚举值
public enum CompareOperator {
  // Keeps same names as the enums over in filter's CompareOp intentionally.
  // The convertion of operator to protobuf representation is via a name comparison.
  /** less than */
  LESS,
  /** less than or equal to */
  LESS_OR_EQUAL,
  /** equals */
  EQUAL,
  /** not equal */
  NOT_EQUAL,
  /** greater than or equal to */
  GREATER_OR_EQUAL,
  /** greater than */
  GREATER,
  /** no operation */
  NO_OP,
}
```

### 69.2.2. ColumnValueFilter

> Introduced in HBase-2.0.0 version as a complementation of SingleColumnValueFilter, ColumnValueFilter gets matched cell only, while SingleColumnValueFilter gets the entire row (has other columns and values) to which the matched cell belongs. Parameters of constructor of ColumnValueFilter are the same as SingleColumnValueFilter.

在 HBase-2.0.0 版本中引入，作为 SingleColumnValueFilter 的补充，

**ColumnValueFilter 只获得匹配的单元格，而 SingleColumnValueFilter 获得匹配单元格所属的整个行(有其他列和值)。**

ColumnValueFilter 构造函数的参数与 SingleColumnValueFilter 相同。

```java
ColumnValueFilter filter = new ColumnValueFilter(
  cf,
  column,
  CompareOperaor.EQUAL,
  Bytes.toBytes("my value")
  );
scan.setFilter(filter);
```

> Note. For simple query like "equals to a family:qualifier:value", we highly recommend to use the following way instead of using SingleColumnValueFilter or ColumnValueFilter:

注意：**对于 "equals to a family:qualifier:value" 这种简单查询，强烈推荐使用下列方式**，而不是使用 SingleColumnValueFilter 或 ColumnValueFilter：

```java
Scan scan = new Scan();
scan.addColumn(Bytes.toBytes("family"), Bytes.toBytes("qualifier"));
ValueFilter vf = new ValueFilter(CompareOperator.EQUAL,
  new BinaryComparator(Bytes.toBytes("value")));
scan.setFilter(vf);
...
```

> This scan will restrict to the specified column 'family:qualifier', avoiding scan unrelated families and columns, which has better performance, and ValueFilter is the condition used to do the value filtering.

**此扫描会限制指定的列 'family:qualifier'，避免扫描不相关的列族和列，具有更好的性能**。ValueFilter 是用来进行值过滤的条件。

> But if query is much more complicated beyond this book, then please make your good choice case by case.

但是，如果查询更复杂，请根据情况选择。

## 69.3. Column Value Comparators

> There are several Comparator classes in the Filter package that deserve special mention. These Comparators are used in concert with other Filters, such as [SingleColumnValueFilter](https://hbase.apache.org/2.2/book.html#client.filter.cv.scvf).

在 Filter 包下存在多种 Comparator 类。这些 Comparator 与其他过滤器一起使用的，如SingleColumnValueFilter。

### 69.3.1. RegexStringComparator

> [RegexStringComparator](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/RegexStringComparator.html) supports regular expressions for value comparisons.

**RegexStringComparator 支持正则表达式比较**。

```java
RegexStringComparator comp = new RegexStringComparator("my.");   // any value that starts with 'my'
SingleColumnValueFilter filter = new SingleColumnValueFilter(
  cf,
  column,
  CompareOperaor.EQUAL,
  comp
  );
scan.setFilter(filter);
```

> See the Oracle JavaDoc for [supported RegEx patterns in Java](http://download.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html).

### 69.3.2. SubstringComparator

> [SubstringComparator](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SubstringComparator.html) can be used to determine if a given substring exists in a value. The comparison is case-insensitive.

**SubstringComparator 用来确定一个给定的子串是否在一个值中**。比较不区分大小写。

```java
SubstringComparator comp = new SubstringComparator("y val");   // looking for 'my value'
SingleColumnValueFilter filter = new SingleColumnValueFilter(
  cf,
  column,
  CompareOperaor.EQUAL,
  comp
  );
scan.setFilter(filter);
```

### 69.3.3. BinaryPrefixComparator

> See [BinaryPrefixComparator](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.html).

【一种比较器，它与指定的字节数组进行比较，但只与该字节数组的长度进行比较。】

### 69.3.4. BinaryComparator

> See [BinaryComparator](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/BinaryComparator.html).

【一个二进制比较器，使用`org.apache.hadoop.hbase.util.Bytes#compareTo(byte[], byte[])`按字典顺序与指定的字节数组进行比较。】

【帮助理解：[https://www.imooc.com/article/303791](https://www.imooc.com/article/303791)】

### 69.3.5. BinaryComponentComparator

> [BinaryComponentComparator](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/BinaryComponentComparator.html) can be used to compare specific value at specific location with in the cell value. The comparison can be done for both ascii and binary data.

**BinaryComponentComparator 用来比较在指定位置的指定值与单元格中的指定值**。可以对 ascii 数据和二进制数据进行比较。

```java
byte[] partialValue = Bytes.toBytes("partial_value");
    int partialValueOffset =
    Filter partialValueFilter = new ValueFilter(CompareFilter.CompareOp.GREATER,
            new BinaryComponentComparator(partialValue,partialValueOffset));
```

> See [HBASE-22969](https://issues.apache.org/jira/browse/HBASE-22969) for other use cases and details.

## 69.4. KeyValue Metadata

> As HBase stores data internally as KeyValue pairs, KeyValue Metadata Filters evaluate the existence of keys (i.e., ColumnFamily:Column qualifiers) for a row, as opposed to values the previous section.

由于 HBase 内部将数据存储为 KeyValue 对，因此 KeyValue Metadata Filters 将为一行评估 keys 是否存在(即ColumnFamily:Column qualifiers)，。。。。

【确定了一个rowkey，通过列族和列名组成的键值对(即ColumnFamily:Column qualifiers)可以确定一个cell】

### 69.4.1. FamilyFilter

> [FamilyFilter](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FamilyFilter.html) can be used to filter on the ColumnFamily. It is generally a better idea to select ColumnFamilies in the Scan than to do it with a Filter.

FamilyFilter 用来在列族上过滤。通过在 Scan 中选择列族要比使用过滤器更好。

【获取指定列族下的所有列】

### 69.4.2. QualifierFilter

> [QualifierFilter](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/QualifierFilter.html) can be used to filter based on Column (aka Qualifier) name.

QualifierFilter 根据列名来取。

### 69.4.3. ColumnPrefixFilter

> [ColumnPrefixFilter](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.html) can be used to filter based on the lead portion of Column (aka Qualifier) names.

ColumnPrefixFilter 用来基于列名的前面部分过滤。

> A ColumnPrefixFilter seeks ahead to the first column matching the prefix in each row and for each involved column family. It can be used to efficiently get a subset of the columns in very wide rows.

ColumnPrefixFilter 在每行和涉及的每个列族中，**查找与前缀匹配的列**。

【根据一个前缀，取列】

它可以用于在非常宽的行中获取列的子集。

> Note: The same column qualifier can be used in different column families. This filter returns all matching columns.

注意:相同的列限定符可以用于不同的列族。此过滤器返回所有匹配的列。

> Example: Find all columns in a row and family that start with "abc"

示例：查找以“abc”开头的所有列

```java
Table t = ...;
byte[] row = ...;
byte[] family = ...;
byte[] prefix = Bytes.toBytes("abc");
Scan scan = new Scan(row, row); // (optional) limit to one row
scan.addFamily(family); // (optional) limit to one family
Filter f = new ColumnPrefixFilter(prefix);
scan.setFilter(f);
scan.setBatch(10); // set this if there could be many columns returned
ResultScanner rs = t.getScanner(scan);
for (Result r = rs.next(); r != null; r = rs.next()) {
  for (KeyValue kv : r.raw()) {
    // each kv represents a column
  }
}
rs.close();
```

```java
package hbase.filter;
.....

public class ColumnPrefixFilterTest {
    public static void main(String[] args) throws Exception {

        Configuration conf = HBaseConfiguration.create();
        conf.set("hbase.rootdir", "hdfs://zgg:9000/user/hbase");
        conf.set("hbase.zookeeper.quorum","zgg");
        conf.set("hbase.zookeeper.property.clientPort","2181");
        conf.set("zookeeper.znode.parent","/hbase");

        Connection connection = ConnectionFactory.createConnection(conf);

        Table table = connection.getTable(TableName.valueOf("user"));
        

        //取前缀为 n 的列名
        byte[] prefix = Bytes.toBytes("n");

        Scan scan = new Scan();

        // 根据前缀来匹配(age,name,occupation,salary)这些列名
        Filter f = new ColumnPrefixFilter(prefix);
        scan.setFilter(f);

        ResultScanner rs = table.getScanner(scan);
        System.out.println("Scanning table user...");

        for (Result result : rs) {
            for (Cell cell : result.rawCells()) {
                System.out.println("Cell: " + cell);
                System.out.println("Value: " +
                        Bytes.toString(cell.getValueArray(), cell.getValueOffset(),
                                cell.getValueLength()));
                //Scanning table user...
                //Cell: 1/info1:name/1608553940007/Put/vlen=3/seqid=0
                //Value: tom
                //Cell: 2/info1:name/1608553940173/Put/vlen=4/seqid=0
                //Value: jack
                //Cell: 3/info1:name/1608553940305/Put/vlen=4/seqid=0
                //Value: mike
                //Cell: 4/info1:name/1608553940436/Put/vlen=6/seqid=0
                //Value: marcus
            }
        }
        rs.close();

    }
}

```

### 69.4.4. MultipleColumnPrefixFilter

> [MultipleColumnPrefixFilter](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.html) behaves like ColumnPrefixFilter but allows specifying multiple prefixes.

MultipleColumnPrefixFilter 的行为类似于 ColumnPrefixFilter，但**允许指定多个前缀**。

【根据多个前缀，取列】

Like ColumnPrefixFilter, MultipleColumnPrefixFilter efficiently seeks ahead to the first column matching the lowest prefix and also seeks past ranges of columns between prefixes. It can be used to efficiently get discontinuous sets of columns from very wide rows.

> Example: Find all columns in a row and family that start with "abc" or "xyz"

示例：查找以“abc”或“xyz”开头的所有列

```java
Table t = ...;
byte[] row = ...;
byte[] family = ...;
byte[][] prefixes = new byte[][] {Bytes.toBytes("abc"), Bytes.toBytes("xyz")};
Scan scan = new Scan(row, row); // (optional) limit to one row
scan.addFamily(family); // (optional) limit to one family
Filter f = new MultipleColumnPrefixFilter(prefixes);
scan.setFilter(f);
scan.setBatch(10); // set this if there could be many columns returned
ResultScanner rs = t.getScanner(scan);
for (Result r = rs.next(); r != null; r = rs.next()) {
  for (KeyValue kv : r.raw()) {
    // each kv represents a column
  }
}
rs.close();
```

### 69.4.5. ColumnRangeFilter

> A [ColumnRangeFilter](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/ColumnRangeFilter.html) allows efficient intra row scanning.

ColumnRangeFilter 允许有效的行内扫描。

> A ColumnRangeFilter can seek ahead to the first matching column for each involved column family. It can be used to efficiently get a 'slice' of the columns of a very wide row. i.e. you have a million columns in a row but you only want to look at columns bbbb-bbdd.

ColumnRangeFilter 可以为每个涉及的列族查找匹配的列。

它可以用来获得非常宽的行的列的“切片”。

【根据给定的范围，取列】

例如，你一行中有一百万个列，但你只想看这些列 bbbb-bbdd。

> Note: The same column qualifier can be used in different column families. This filter returns all matching columns.

注意：相同的列限定符可以用于不同的列族。此过滤器返回所有匹配的列。

> Example: Find all columns in a row and family between "bbbb" (inclusive) and "bbdd" (inclusive)

示例：在一行中和在"bbbb"(包括)和"bbdd"(包括)列之间，查找所有列。

```java
Table t = ...;
byte[] row = ...;
byte[] family = ...;
byte[] startColumn = Bytes.toBytes("bbbb");
byte[] endColumn = Bytes.toBytes("bbdd");
Scan scan = new Scan(row, row); // (optional) limit to one row
scan.addFamily(family); // (optional) limit to one family
Filter f = new ColumnRangeFilter(startColumn, true, endColumn, true);
scan.setFilter(f);
scan.setBatch(10); // set this if there could be many columns returned
ResultScanner rs = t.getScanner(scan);
for (Result r = rs.next(); r != null; r = rs.next()) {
  for (KeyValue kv : r.raw()) {
    // each kv represents a column
  }
}
rs.close();
```

```java
//帮助理解
package hbase.filter;
....

public class ColumnRangeFilterTest {
    public static void main(String[] args) throws Exception {

        Configuration conf = HBaseConfiguration.create();
        conf.set("hbase.rootdir", "hdfs://zgg:9000/user/hbase");
        conf.set("hbase.zookeeper.quorum","zgg");
        conf.set("hbase.zookeeper.property.clientPort","2181");
        conf.set("zookeeper.znode.parent","/hbase");

        Connection connection = ConnectionFactory.createConnection(conf);

        Table table = connection.getTable(TableName.valueOf("user"));

        //取的开始、结束行
        byte[] startRow = Bytes.toBytes("1");
        byte[] stopRow = Bytes.toBytes("2");

        //取的一个列族、和开始、结束列
        byte[] family = Bytes.toBytes("info1");
        byte[] startColumn = Bytes.toBytes("age");
        byte[] stopColumn = Bytes.toBytes("name");

        // (optional) 限制为第一行和第二行
        Scan scan = new Scan().withStartRow(startRow,true).withStopRow(stopRow,true);
        // (optional) 限制为一个列族info1
        scan.addFamily(family);

        // 选择那些在 minColumn 到 maxColumn 之间的列的所在 key。【参考结果来理解】
        Filter f = new ColumnRangeFilter(startColumn, true, stopColumn, true);
        scan.setFilter(f);

//        scan.setBatch(10); // set this if there could be many columns returned
        ResultScanner rs = table.getScanner(scan);
        for (Result result : rs) {
            for (Cell cell : result.rawCells()) {
                System.out.println("Cell: " + cell);
                System.out.println("Value: " +
                        Bytes.toString(cell.getValueArray(), cell.getValueOffset(),
                                cell.getValueLength()));
                //Cell: 1/info1:age/1608553940055/Put/vlen=2/seqid=0
                //Value: 22
                //Cell: 1/info1:name/1608553940007/Put/vlen=3/seqid=0
                //Value: tom
                //Cell: 2/info1:age/1608553940207/Put/vlen=2/seqid=0
                //Value: 20
                //Cell: 2/info1:name/1608553940173/Put/vlen=4/seqid=0
                //Value: jack
            }
        }
        rs.close();

    }
}

```

> Note: Introduced in HBase 0.92

## 69.5. RowKey

### 69.5.1. RowFilter

> It is generally a better idea to use the startRow/stopRow methods on Scan for row selection, however [RowFilter](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/RowFilter.html) can also be used.

**在 `Scan` 上使用 `startRow/stopRow` 来选择行是一个好主意。然而，也可以使用 `RowFilter`**。

> You can supplement a scan (both bounded and unbounded) with RowFilter constructed from [BinaryComponentComparator](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/BinaryComponentComparator.html) for further filtering out or filtering in rows. See [HBASE-22969](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/BinaryComponentComparator.html) for use cases and other details.

可以使用由 `BinaryComponentComparator` 构造的 `RowFilter` 来补充一个 scan(有界的和无界的)，以便进一步过滤出或按行过滤。【？？？】

## 69.6. Utility

### 69.6.1. FirstKeyOnlyFilter

> This is primarily used for rowcount jobs. See [FirstKeyOnlyFilter](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html).

这主要**用作统计行数**。